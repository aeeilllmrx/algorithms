{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is one of the most commonly used algorithms in machine learning, both because of it's interpretability and because (with some modifications, discussed below) it can have incredibly accuracy. Here's an implementation using a dataset with several categorical variables and a target representing whether the individual will carry out a violent act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLACE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NEURO</th>\n",
       "      <th>EMOT</th>\n",
       "      <th>DANGER</th>\n",
       "      <th>BEHAV</th>\n",
       "      <th>VIOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PLACE  RACE  GENDER  NEURO  EMOT  DANGER  BEHAV  VIOL\n",
       "0      1     0       0      3     1       0      0     0\n",
       "1      3     1       1      0     0       1      7     1\n",
       "2      0     1       0      0     0       1      4     0\n",
       "3      0     0       1      0     0       3      6     1\n",
       "4      0     0       1      2     1       3      7     1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "df = pd.read_csv(\"data/aps.csv\")\n",
    "\n",
    "data = df[['PLACE', 'RACE', 'GENDER', 'NEURO', 'EMOT', 'DANGER', 'BEHAV', 'VIOL']]\n",
    "\n",
    "cutoff = int(.8 * len(data))\n",
    "data_train = data[:cutoff]; data_test = data[cutoff:]\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start figuring out how the decision tree works, we need a way to represent that tree. One way is by using two dict of dicts, one to represent nodes and another to represent leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(dict):\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return False\n",
    "\n",
    "class Leaf(dict):\n",
    "    def __init__(self, target):\n",
    "        self.target = target\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started building our tree, we'll need an entropy function, something that tells us how varied our data is. Intuitively, we want to make splits in our tree on features that have the widest range of target values, so that we get the most \"information\" from a new data point at each step along the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        start up the tree\n",
    "        '''\n",
    "        self.root = None\n",
    "        \n",
    "    def fit(self, data_train):\n",
    "        '''\n",
    "        generates feature list and calls create_tree\n",
    "        we need this method to define the feature list before we begin building \n",
    "        '''\n",
    "        data_train = np.array(data_train)\n",
    "        feature_list = [i for i in range(len(data_train[0]))]\n",
    "        self.root = self.create_tree(np.array(data_train), feature_list)\n",
    "                                                \n",
    "    def target_entropy(self, values):\n",
    "        '''\n",
    "        the entropy for a given feature's label values\n",
    "        '''\n",
    "        if len(set(values)) <= 1:\n",
    "            return 1\n",
    "        size = float(len(values))\n",
    "        classes = Counter(values)\n",
    "        probs = [i / size for i in classes.values()]\n",
    "        entropy = np.sum([-probs[i]*np.log(probs[i]) for i in range(len(probs))])\n",
    "        \n",
    "        return entropy\n",
    "        \n",
    "    def data_entropy(self, train_data):\n",
    "        '''\n",
    "        the entropy for any chunk of data- calls self.entropy\n",
    "        '''\n",
    "        data = np.array(train_data)\n",
    "        target_col = data[:,-1]\n",
    "\n",
    "        return self.target_entropy(target_col)\n",
    "\n",
    "    def best_feature(self, train_data, feature_list):\n",
    "        '''\n",
    "        returns the feature to split on, or -1 if we can't improve\n",
    "        '''\n",
    "        best_entropy = -100\n",
    "        base_entropy = self.data_entropy(train_data)\n",
    "        for f in feature_list:\n",
    "            \n",
    "            unique_vals = list(set(train_data.T[f]))\n",
    "            parts = self.split(train_data, f, unique_vals)\n",
    "            n = sum([len(p) for p in parts])\n",
    "           \n",
    "            props = [float(len(p)) / n for p in parts]\n",
    "            new_entropy = np.sum([props[i] * self.data_entropy(p) for i, p in enumerate(parts)])\n",
    "            \n",
    "            entropy_change = base_entropy - new_entropy\n",
    "\n",
    "            if entropy_change > best_entropy:\n",
    "                best_entropy = entropy_change\n",
    "                split_feature = f\n",
    "         \n",
    "        feature_list.pop(feature_list.index(split_feature)) \n",
    "        return split_feature\n",
    "        \n",
    "    def split(self, train_data, f, unique_vals):\n",
    "        '''\n",
    "        split into groups for each value the feature can take on\n",
    "        '''\n",
    "        parts = [[row for row in train_data if row[f] == v] for v in unique_vals]\n",
    "        return parts\n",
    "    \n",
    "    def get_most_common(self, train_data):\n",
    "        '''\n",
    "        returns the most common value for a chunk of data\n",
    "        '''\n",
    "        target_col = train_data[:,-1]\n",
    "        classes = Counter(target_col)\n",
    "        return classes.most_common(1)[0][0] # indices are most common, then actual value\n",
    "    \n",
    "    def create_tree(self, train_data, feature_list):\n",
    "        '''\n",
    "        recursively create the decision tree\n",
    "        '''\n",
    "        if len(feature_list) == 0:\n",
    "            val = self.get_most_common(train_data)\n",
    "            root = Leaf(val)\n",
    "        \n",
    "        else:\n",
    "            best_feature = self.best_feature(train_data, feature_list)\n",
    "            print \"feature chosen: \", best_feature\n",
    "            root = Node(best_feature)\n",
    "            unique_vals = list(set(train_data.T[best_feature]))\n",
    "            \n",
    "            for value in unique_vals:\n",
    "                subdata = np.array([row for row in train_data if row[best_feature] == value])\n",
    "                child = self.create_tree(subdata, feature_list)\n",
    "                root[value] = child\n",
    "\n",
    "        return root\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        '''\n",
    "        traverse the tree we created to predict a new data point\n",
    "        '''\n",
    "        labels = []\n",
    "\n",
    "        for t in test_data:\n",
    "            target = None\n",
    "            current_node = self.root\n",
    "            while target is None:\n",
    "                if current_node.is_leaf():\n",
    "                    target = current_node.target\n",
    "                else:\n",
    "                    key_value = t[current_node.feature]\n",
    "                    current_node = current_node[key_value]\n",
    "            labels.append(target)\n",
    "        return labels\n",
    "    \n",
    "    def score(self, test_data):\n",
    "        '''\n",
    "        evaluate the accuracy of that prediction\n",
    "        '''\n",
    "        preds = np.array(self.predict(np.array(test_data)))\n",
    "        actual_targets = np.array(test_data)[:, -1]\n",
    "        accuracy = np.mean(preds == actual_targets)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature chosen:  5\n",
      "feature chosen:  0\n",
      "feature chosen:  1\n",
      "feature chosen:  2\n",
      "feature chosen:  3\n",
      "feature chosen:  4\n",
      "feature chosen:  6\n",
      "feature chosen:  7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84313725490196079"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = DecisionTree()\n",
    "\n",
    "d.fit(data_train)\n",
    "d.score(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we can see how bagging, random forests, and boosting are all techniques that involve relatively painless modifications of our original class.\n",
    "\n",
    "Bagging simply involves making multiple trees from different portions of the dataset and averaging out all the examples. Random forests is similar, except we only use a specific subset of the features. Finally, boosting means that for each tree we make, we take note of the data points which were incorrectly classified and adjust our loss function accordingly.\n",
    "\n",
    "In each of these cases, we'll want to keep a basic version of our decision tree that has the entropy, splitting, create tree, and score functions, and add methods to deal with our expanded options. I'll be including the BaseDecisionTree code soon, but here is how this would work for a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RF(BaseDecisionTree):\n",
    "    \n",
    "    def __init__(self, nbags=5, d_portion=0.2, f_portion=0.5):\n",
    "        '''\n",
    "        initialize variables\n",
    "        '''\n",
    "        \n",
    "        self.root = [None] * nbags\n",
    "        self.n = nbags\n",
    "        self.data_portion = d_portion\n",
    "        self.feature_portion = f_portion\n",
    "            \n",
    "    def get_portion(self, data_train):\n",
    "        '''\n",
    "        only select a subset of data\n",
    "        '''\n",
    "        \n",
    "        n_data = int(self.data_portion * len(data_train))\n",
    "        data_part = np.random.permutation(np.array(data_train))[:n]\n",
    "        \n",
    "        n_feat = int(self.feature_portion * (len(data_train.T) - 1))\n",
    "        total_feature_list = [i for i in range(len(data_train[0]))]\n",
    "        feature_part = np.random.permutation(np.array(total_feature_list))[:n]\n",
    "        \n",
    "        return data_part, feature_part\n",
    "        \n",
    "    def fit(self, data_train):\n",
    "        '''\n",
    "        only select a subset of features\n",
    "        '''\n",
    "        \n",
    "        data_part, feature_part = get_portion(data_train)\n",
    "        for i in range(self.n):\n",
    "            self.root[i] = self.create_tree(data_part, feature_part)\n",
    "    \n",
    "    def average_predictions(self, test_data):\n",
    "        '''\n",
    "        use the predict method but average out all the results\n",
    "        '''\n",
    "        predictions = []\n",
    "        for t in test_data:\n",
    "            tree_preds = [predict(t, self.root[i]) for i in range(self.nbags)]\n",
    "            predictions.append(get_most_common(tree_preds), single_list=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If you're interested in the details, I highly recommend checking out the book \"Ensemble Methods in Data Mining\" by Seni and Elder, which has a nice explanation of how these kinds of ensemble methods have developed and what they all have in common.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
