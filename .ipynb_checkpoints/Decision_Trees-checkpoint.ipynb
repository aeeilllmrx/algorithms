{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is one of the most commonly used algorithms in machine learning, both because of it's interpretability and because (with some modifications, discussed below) it can have incredibly accuracy. Here's a simple implementation, using a dataset with several categorical variables and a target representing whether the individual will carry out a violent act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLACE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NEURO</th>\n",
       "      <th>EMOT</th>\n",
       "      <th>DANGER</th>\n",
       "      <th>BEHAV</th>\n",
       "      <th>AGE_BIN</th>\n",
       "      <th>VIOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(15.179, 16.55]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>(13.807, 15.179]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>(15.179, 16.55]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>(15.179, 16.55]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>(15.179, 16.55]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PLACE  RACE  GENDER  NEURO  EMOT  DANGER  BEHAV           AGE_BIN  VIOL\n",
       "0      1     0       0      3     1       0      0   (15.179, 16.55]     0\n",
       "1      3     1       1      0     0       1      7  (13.807, 15.179]     1\n",
       "2      0     1       0      0     0       1      4   (15.179, 16.55]     0\n",
       "3      0     0       1      0     0       3      6   (15.179, 16.55]     1\n",
       "4      0     0       1      2     1       3      7   (15.179, 16.55]     1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/aps.csv\")\n",
    "df['AGE_BIN'] = pd.cut(df.AGE, 5)\n",
    "\n",
    "# target = df.VIOL\n",
    "# data = df.drop([\"AGE\",\"ID\",\"PLACE3\",\"LOS\",\"ELOPE\",\"CUSTD\"], axis =1)\n",
    "\n",
    "data = df[['PLACE', 'RACE', 'GENDER', 'NEURO', 'EMOT', 'DANGER', 'BEHAV','AGE_BIN', 'VIOL']]\n",
    "\n",
    "cutoff = int(.8 * len(data))\n",
    "data_train = data[:cutoff]; data_test = data[cutoff:]\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start figuring out how the decision tree works, we need a way to represent that tree. One straightforward way is through a dict of dicts, as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class Tree_LL():\n",
    "\n",
    "    def __init__(self, feature, val):\n",
    "        self.current_node = [feature, val]\n",
    "        self.left_child = []\n",
    "        self.right_child = []\n",
    "    \n",
    "    def insert_left(self, feature, val):\n",
    "        self.left_child = Tree_LL(feature, val)\n",
    "    \n",
    "    def insert_right(self, feature, val):\n",
    "        self.right_child = Tree_LL(feature, val)\n",
    "        \n",
    "    def evaluate(self, data_point):\n",
    "        \n",
    "        while (self.left_child or self.right_child):\n",
    "            \n",
    "            feature = self.current_node[0] \n",
    "            \n",
    "            if data_point[feature] == self.current_node[1]:\n",
    "                self.current_node = self.current_node.left_child\n",
    "                self.evaluate(data_point)\n",
    "            elif data_point[feature] == self.current_node[1]:\n",
    "                self.current_node = self.current_node.right_child\n",
    "                self.evaluate(data_point)\n",
    "            \n",
    "        return self.current_node.most()\n",
    "            \n",
    "    def most(self):\n",
    "        \n",
    "        counts = Counter(self.current_node)\n",
    "        most_common_label = counts.most_common(1)\n",
    "        return most_common_label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of implementing decision trees is to have three functions: one that computes entropy, one that chooses the best feature and value to split on, and one to actually carry out the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.num_regions = 1\n",
    "        self.regions = [self.data]\n",
    "        self.features = list(data.columns)\n",
    "        self.splits = []\n",
    "        self.tree_rep = Tree(None)\n",
    "    \n",
    "    \n",
    "    def entropy(self, region):\n",
    "        \n",
    "        # if there's nothing in this region, entropy is 1\n",
    "        if len(region) == 0:\n",
    "            return 1\n",
    "        \n",
    "        target_col = region.T[-1]\n",
    "        size = float(len(target_col))\n",
    "        classes = Counter(target_col)\n",
    "        \n",
    "        # also if there's only one class, entropy is 1\n",
    "        if len(classes) == 1:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            probs = [i / size for i in classes.values()]\n",
    "            entropy = np.sum([-probs[i]*np.log(probs[i]) for i in range(len(probs))])\n",
    "    \n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    def make_new_split(self, regions):\n",
    "        \n",
    "        split_feature = -1; best_entropy = 0.0\n",
    "        \n",
    "        for r, region in enumerate(self.regions):\n",
    "            \n",
    "            base_entropy = self.entropy(region)\n",
    "            for f, feature in enumerate(region.T):\n",
    "                \n",
    "                unique_vals = list(set(feature))\n",
    "                for val in unique_vals:\n",
    "                    \n",
    "                    left, right = self.split(r, f, val)\n",
    "                    prop_left = float(len(left)) / (len(left) + len(right))\n",
    "                    prop_right = 1 - prop_left\n",
    "            \n",
    "                    e_1 = prop_left * self.entropy(left)\n",
    "                    e_2 = prop_right * self.entropy(right)\n",
    "                    \n",
    "                    entropy_change = base_entropy - e_1 - e_2\n",
    "                    if entropy_change > best_entropy:\n",
    "                        \n",
    "                        best_entropy = entropy_change\n",
    "                        split_region = r; split_feature = f; split_val = val\n",
    "        \n",
    "        if split_feature != -1:\n",
    "            \n",
    "            self.regions.extend(self.split(split_region, split_feature, split_val))\n",
    "            del self.regions[split_region] \n",
    "            self.num_regions += 1\n",
    "            \n",
    "            return split_region, split_feature, split_val\n",
    "\n",
    "        \n",
    "    def split(self, r, f, val):\n",
    "        \n",
    "        left = np.array([row for row in self.regions[r] if row[f] == val])\n",
    "        right = np.array([row for row in self.regions[r] if row[f] != val])\n",
    "        \n",
    "        return left, right\n",
    "    \n",
    "    \n",
    "    def most(self, region):\n",
    "        \n",
    "        counts = Counter(region)\n",
    "        most_common_label = max(counts, key=lambda x: x[1])[0]\n",
    "        \n",
    "        return most_common_label\n",
    "\n",
    "    \n",
    "    def create_tree(self):\n",
    "        \n",
    "        # we can always alter the stopping criteria\n",
    "        while self.num_regions < 10:\n",
    "            \n",
    "            # not sure how this will work\n",
    "            region, feature, value = self.make_new_split(self.regions)\n",
    "            self.tree_rep[region][feature] = value\n",
    "            \n",
    "        # finally, loop through leaves and apply most() to them?\n",
    "            \n",
    "        # print self.num_regions\n",
    "        # print dt\n",
    "            \n",
    "            \n",
    "    def classify(self, test_data):\n",
    "        \n",
    "        predictions = []\n",
    "        # for line in test_data:\n",
    "            # traverse the tree\n",
    "            # predictions.append(final_leaf)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    def evaluate(self, predictions, test_labels):\n",
    "    \n",
    "        return np.mean(predictions == target_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = DecisionTree(data_train)\n",
    "d.create_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"0\": {\"5\": 0}, \"1\": {\"0\": 3, \"5\": 1}, \"2\": {\"2\": 1, \"6\": 4}, \"4\": {\"4\": 0, \"6\": 3}, \"6\": {\"3\": 2}, \"7\": {\"6\": 3}}\n"
     ]
    }
   ],
   "source": [
    "print d.tree_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is done, we can see how bagging, random forests, and boosting are all techniques that involve relatively painless modifications of our original class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging simply involves making multiple trees from different portions of the dataset and averaging out all the examples. Random forests is similar, except we only use a specific subset of the features. Finally, boosting means that for each tree we make, we take note of the data points which were incorrectly classified and adjust our loss function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# can put this into a class later\n",
    "\n",
    "def get_random_sample(data, frac):\n",
    "    permuted = np.random.permutation(data)\n",
    "    rows = int(len(data) * frac)\n",
    "    return permuted[: rows]\n",
    "\n",
    "def make_feature_subset(data, num_features):\n",
    "    \n",
    "    columns = np.random.choice(data.shape[1] - 1, num_features, replace=False)\n",
    "    columns = np.append(columns, data[:, -1])\n",
    "    \n",
    "    return data[:, columns]\n",
    "    \n",
    "def make_trees(data, num_trees, num_features, loss_function=None):\n",
    "    \n",
    "    bags = []\n",
    "\n",
    "    # if num_features is a subset, it's random forest\n",
    "    if num_features < data.shape[1]:\n",
    "        data = make_feature_subset(data)\n",
    "    \n",
    "    # boosted trees apply an exponential loss to deal with misclassified example\n",
    "    if loss_function is not None:\n",
    "        \n",
    "        # need to update this in the class itself\n",
    "        return 0\n",
    "    \n",
    "    # bagging uses several trees\n",
    "    for i in range(num_trees):\n",
    "        data_sample = get_random_sample(data)\n",
    "        dt = DecisionTree(data_sample)\n",
    "        bags.append(dt)\n",
    "        \n",
    "def most(values):\n",
    "    \n",
    "    counts = Counter(values)\n",
    "    most_common_label = counts.most_common(1)\n",
    "    return most_common_label\n",
    "    \n",
    "def predict(test_data, bags):\n",
    "    num_bags = len(bags)\n",
    "    preds = np.empty((num_bags, len(test_data)))\n",
    "    for i in range(num_bags):\n",
    "        this_pred = bags[i].classify(test_data)\n",
    "        preds[i] = this_pred\n",
    "    return [most(i) for i in preds.T] # not sure if transpose is needed here\n",
    "                     \n",
    "def evaluate(predictions, actual_y):\n",
    "    \n",
    "    # same as above\n",
    "    return np.mean(predictions == target_test)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
